{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this work is to process a text dataset using Neural Networks and Deep Learning\n",
    "word embedding and data analytics methods and to extract knowledge from it. Prepare a report\n",
    "for this work and deposit it on moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this work you will use 20 Newsgroup dataset, but you a free to use any text data (UCI datasets\n",
    "repository, kaggle, data.gouv.fr, â€¦) informing the Professor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work should contains at least the following 4 parts:\n",
    "1. Analysis of the text dataset\n",
    "2. Text processing and Transformation\n",
    "3. Apply di erent Neural Networks (NN) embedding techniques\n",
    "4. Clustering and/or classi cation on the embedded data\n",
    "5. Results analysis and visualisation\n",
    "6. Theoretical formalism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this work you will use 20 Newsgroup dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "Organization: The City University\n",
      "Lines: 14\n",
      "\n",
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "-- \n",
      "Michael Collier (Programmer)                 The Computer Unit,\n",
      "Email: M.P.Collier@uk.ac.city                The City University,\n",
      "Tel: 071 477-8000 x3769                      London,\n",
      "Fax: 071 477-8565                            EC1V 0HB.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyse the dataset : the context, size, difficulties, detect the objectives.\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "\n",
    "# Load the 20 newsgroups dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# the context of the dataset\n",
    "print(newsgroups_train.target_names)\n",
    "print(newsgroups_train.data[0])\n",
    "\n",
    "X_train = newsgroups_train.data\n",
    "Y_train = newsgroups_train.target\n",
    "\n",
    "X_test = newsgroups_test.data\n",
    "Y_test = newsgroups_test.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2257\n",
      "1502\n"
     ]
    }
   ],
   "source": [
    "# analyse the size of the dataset\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Processing and Transformation\n",
    "# For this part, you should use scikit-learn and you can follow the tutorial:\n",
    "# https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#tutorial-setup\n",
    "\n",
    "# Assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices)\n",
    "\n",
    "# For each document #i, count the number of occurrences of each word w and store it in X[i, j] as the value of feature #j where j is the index of word w in the dictionary\n",
    "def build_X(data, dictionary):\n",
    "    X = np.zeros((len(data), len(dictionary)), dtype=np.int)\n",
    "    for i, doc in enumerate(data):\n",
    "        for word in doc.split():\n",
    "            X[i, dictionary[word]] += 1\n",
    "    return X\n",
    "\n",
    "\n",
    "def build_dictionary(data):\n",
    "    dictionary = {}\n",
    "    for doc in data:\n",
    "        for word in doc.split():\n",
    "            if word not in dictionary:\n",
    "                dictionary[word] = len(dictionary)\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "dictionary_train = build_dictionary(X_train)\n",
    "X_bow_train = build_X(X_train, dictionary_train)\n",
    "\n",
    "dictionary_test = build_dictionary(X_test)\n",
    "X_bow_test = build_X(X_test, dictionary_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(data, dictionary):\n",
    "#     vectorizer = CountVectorizer(vocabulary=dictionary)\n",
    "#     X = vectorizer.fit_transform(data)\n",
    "#     return X\n",
    "\n",
    "# X_cv_train = tokenize(X_train, dictionary_train)\n",
    "\n",
    "\n",
    "# print(X_cv_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 77)\t0.1501782300675281\n",
      "  (0, 76)\t0.1501782300675281\n",
      "  (0, 75)\t0.1501782300675281\n",
      "  (0, 74)\t0.103699862561056\n",
      "  (0, 73)\t0.12962948804367816\n",
      "  (0, 72)\t0.1501782300675281\n",
      "  (0, 71)\t0.1501782300675281\n",
      "  (0, 70)\t0.2851886015699785\n",
      "  (0, 69)\t0.11378139920465326\n",
      "  (0, 68)\t0.07044791025673265\n",
      "  (0, 67)\t0.1501782300675281\n",
      "  (0, 66)\t0.11378139920465326\n",
      "  (0, 65)\t0.12962948804367816\n",
      "  (0, 64)\t0.05877826376929799\n",
      "  (0, 63)\t0.1501782300675281\n",
      "  (0, 62)\t0.1501782300675281\n",
      "  (0, 61)\t0.080491850362952\n",
      "  (0, 60)\t0.035036250633131864\n",
      "  (0, 59)\t0.1501782300675281\n",
      "  (0, 58)\t0.09774341765798598\n",
      "  (0, 57)\t0.02293400626289806\n",
      "  (0, 56)\t0.06833288469445191\n",
      "  (0, 55)\t0.1330396798598353\n",
      "  (0, 54)\t0.08004648953595733\n",
      "  (0, 53)\t0.029526882905000593\n",
      "  :\t:\n",
      "  (2256, 437)\t0.04674830046617758\n",
      "  (2256, 434)\t0.10723383440124384\n",
      "  (2256, 433)\t0.07377977334740078\n",
      "  (2256, 378)\t0.06554613623435271\n",
      "  (2256, 339)\t0.06900722582663284\n",
      "  (2256, 272)\t0.04838252718062753\n",
      "  (2256, 270)\t0.037741501674035165\n",
      "  (2256, 265)\t0.04344266470184877\n",
      "  (2256, 184)\t0.044314928112002626\n",
      "  (2256, 182)\t0.031404629725128745\n",
      "  (2256, 165)\t0.03190651056967081\n",
      "  (2256, 111)\t0.028665716635931884\n",
      "  (2256, 103)\t0.05083456263322761\n",
      "  (2256, 95)\t0.02869170389899093\n",
      "  (2256, 64)\t0.07333525400609109\n",
      "  (2256, 60)\t0.04371330785962891\n",
      "  (2256, 53)\t0.03683949336008413\n",
      "  (2256, 52)\t0.07099126499186287\n",
      "  (2256, 41)\t0.10002891457177429\n",
      "  (2256, 22)\t0.05093382302168642\n",
      "  (2256, 18)\t0.09584591684747919\n",
      "  (2256, 17)\t0.02342976119884394\n",
      "  (2256, 13)\t0.024698422778297\n",
      "  (2256, 4)\t0.023336559815436305\n",
      "  (2256, 0)\t0.023336559815436305\n"
     ]
    }
   ],
   "source": [
    "def tfidf(X):\n",
    "    transformer = TfidfTransformer()\n",
    "    X = transformer.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "X_tfidf_train = tfidf(X_bow_train)\n",
    "\n",
    "print(X_tfidf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ion\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "data = pd.DataFrame(X_train, columns=['Text'])\n",
    "\n",
    "def get_corpus(data):\n",
    "    corpus_text = 'n'.join(data[:1000]['Text'])\n",
    "    data = []\n",
    "    # iterate through each sentence in the file\n",
    "    for i in sent_tokenize(corpus_text):\n",
    "        temp = []\n",
    "        # tokenize the sentence into words\n",
    "        for j in word_tokenize(i):\n",
    "            temp.append(j.lower())\n",
    "        data.append(temp)\n",
    "    return data\n",
    "\n",
    "\n",
    "corpus = get_corpus(data)\n",
    "\n",
    "# Word2Vec\n",
    "model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# get the vector for each word in the vocabulary\n",
    "words_key_to_index = model.wv.key_to_index\n",
    "words_index_to_key = model.wv.index_to_key\n",
    "words_vectors = model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "\n",
    "# def build_doc2vec(data):\n",
    "#     documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(data)]\n",
    "#     model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def build_doc2vec_X(data, model):\n",
    "#     X = np.zeros((len(data), 5))\n",
    "#     for i, doc in enumerate(data):\n",
    "#         X[i] = model.infer_vector(doc.split())\n",
    "#     return X\n",
    "\n",
    "\n",
    "# model = build_doc2vec(X_bow_train)\n",
    "\n",
    "# X_Doc2Vec = build_doc2vec_X(X_bow_train, model)\n",
    "# print(X_Doc2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101.   1.   1. ...   0.   0. 102.]\n",
      " [101.   1.   0. ...   0.   0. 102.]\n",
      " [101.   1.   0. ...   0.   0. 102.]\n",
      " ...\n",
      " [101.   1.   0. ...   0.   0. 102.]\n",
      " [101.   1.   0. ...   0.   1. 102.]\n",
      " [101.   1.   0. ...   0.   0. 102.]]\n"
     ]
    }
   ],
   "source": [
    "# BERT model\n",
    "def build_bert_X(data):\n",
    "    from transformers import BertTokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    X = np.zeros((len(data), 768))\n",
    "    for i, doc in enumerate(data):\n",
    "        X[i] = tokenizer.encode(doc, add_special_tokens=True, max_length=768, pad_to_max_length=True)\n",
    "    return X\n",
    "\n",
    "X_bow_train_list = X_bow_train.tolist()\n",
    "\n",
    "X_bert = build_bert_X(X_bow_train_list)\n",
    "print(X_bert)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ffc50327917d0330237aeed608577229a5182ed624d0b41d3fc311b6ae6efbd2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
