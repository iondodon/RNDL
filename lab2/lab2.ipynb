{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this work is to process a text dataset using Neural Networks and Deep Learning\n",
    "word embedding and data analytics methods and to extract knowledge from it. Prepare a report\n",
    "for this work and deposit it on moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this work you will use 20 Newsgroup dataset, but you a free to use any text data (UCI datasets\n",
    "repository, kaggle, data.gouv.fr, â€¦) informing the Professor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work should contains at least the following 4 parts:\n",
    "1. Analysis of the text dataset\n",
    "2. Text processing and Transformation\n",
    "3. Apply di erent Neural Networks (NN) embedding techniques\n",
    "4. Clustering and/or classi cation on the embedded data\n",
    "5. Results analysis and visualisation\n",
    "6. Theoretical formalism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this work you will use 20 Newsgroup dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "Organization: The City University\n",
      "Lines: 14\n",
      "\n",
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "-- \n",
      "Michael Collier (Programmer)                 The Computer Unit,\n",
      "Email: M.P.Collier@uk.ac.city                The City University,\n",
      "Tel: 071 477-8000 x3769                      London,\n",
      "Fax: 071 477-8565                            EC1V 0HB.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyse the dataset : the context, size, difficulties, detect the objectives.\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "\n",
    "# Load the 20 newsgroups dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# the context of the dataset\n",
    "print(newsgroups_train.target_names)\n",
    "print(newsgroups_train.data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2257\n",
      "1502\n"
     ]
    }
   ],
   "source": [
    "# analyse the size of the dataset\n",
    "print(len(newsgroups_train.data))\n",
    "print(len(newsgroups_test.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 1 1 1]\n",
      " [1 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Text Processing and Transformation\n",
    "# For this part, you should use scikit-learn and you can follow the tutorial:\n",
    "# https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#tutorial-setup\n",
    "\n",
    "# Assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices)\n",
    "\n",
    "\n",
    "def build_dictionary(data):\n",
    "    \"\"\"\n",
    "    Build a dictionary from words to integer indices.\n",
    "    :param data: a list of documents\n",
    "    :return: a dictionary from words to integer indices\n",
    "    \"\"\"\n",
    "    dictionary = {}\n",
    "    for doc in data:\n",
    "        for word in doc.split():\n",
    "            if word not in dictionary:\n",
    "                dictionary[word] = len(dictionary)\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "dictionary = build_dictionary(newsgroups_train.data)\n",
    "\n",
    "\n",
    "# For each document #i, count the number of occurrences of each word w and store it in X[i, j] as the value of feature #j where j is the index of word w in the dictionary\n",
    "def build_X(data, dictionary):\n",
    "    \"\"\"\n",
    "    Build the feature matrix X from the training set.\n",
    "    :param data: a list of documents\n",
    "    :param dictionary: a dictionary from words to integer indices\n",
    "    :return: the feature matrix X\n",
    "    \"\"\"\n",
    "    X = np.zeros((len(data), len(dictionary)), dtype=np.int)\n",
    "    for i, doc in enumerate(data):\n",
    "        for word in doc.split():\n",
    "            X[i, dictionary[word]] += 1\n",
    "    return X\n",
    "\n",
    "X = build_X(newsgroups_train.data, dictionary)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing text with scikit-learn\n",
    "\n",
    "\n",
    "def build_X_scikit(data, dictionary):\n",
    "    \"\"\"\n",
    "    Build the feature matrix X from the training set.\n",
    "    :param data: a list of documents\n",
    "    :param dictionary: a dictionary from words to integer indices\n",
    "    :return: the feature matrix X\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(vocabulary=dictionary)\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    return X\n",
    "\n",
    "\n",
    "X = build_X_scikit(newsgroups_train.data, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply different embedding techniques based on Neural Networks\n",
    "\n",
    "# import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def build_X_tfidf(data, dictionary):\n",
    "    \"\"\"\n",
    "    Build the feature matrix X from the training set.\n",
    "    :param data: a list of documents\n",
    "    :param dictionary: a dictionary from words to integer indices\n",
    "    :return: the feature matrix X\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(vocabulary=dictionary)\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    return X\n",
    "\n",
    "\n",
    "X = build_X_tfidf(newsgroups_train.data, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should test different embedding approaches: word2vec, FastText, document2vec, BERT, Glove ??????????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[302   0   0 178]\n",
      " [  1 569   2  12]\n",
      " [  0   3 584   7]\n",
      " [  0   1   0 598]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.63      0.77       480\n",
      "           1       0.99      0.97      0.98       584\n",
      "           2       1.00      0.98      0.99       594\n",
      "           3       0.75      1.00      0.86       599\n",
      "\n",
      "    accuracy                           0.91      2257\n",
      "   macro avg       0.93      0.90      0.90      2257\n",
      "weighted avg       0.93      0.91      0.91      2257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "def build_model(X, y):\n",
    "    \"\"\"\n",
    "    Build a Multinomial Naive Bayes model.\n",
    "    :param X: the feature matrix\n",
    "    :param y: the target vector\n",
    "    :return: a MultinomialNB model\n",
    "    \"\"\"\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(X, newsgroups_train.target)\n",
    "\n",
    "\n",
    "def predict(model, X):\n",
    "    \"\"\"\n",
    "    Predict the labels of the documents in X.\n",
    "    :param model: a MultinomialNB model\n",
    "    :param X: the feature matrix\n",
    "    :return: the predicted labels\n",
    "    \"\"\"\n",
    "    return model.predict(X)\n",
    "\n",
    "\n",
    "predicted = predict(model, X)\n",
    "\n",
    "\n",
    "def evaluate(y, predicted):\n",
    "    \"\"\"\n",
    "    Evaluate the model by computing the confusion matrix and the classification report.\n",
    "    :param y: the true labels\n",
    "    :param predicted: the predicted labels\n",
    "    :return: the confusion matrix and the classification report\n",
    "    \"\"\"\n",
    "    print(confusion_matrix(y, predicted))\n",
    "    print(classification_report(y, predicted))\n",
    "\n",
    "\n",
    "evaluate(newsgroups_train.target, predicted)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5328ad86473b9d9451a85e8e3c047fbfd3cd6b0ffc40448c02e5024d3d4459e7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
