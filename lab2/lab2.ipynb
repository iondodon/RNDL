{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this work is to process a text dataset using Neural Networks and Deep Learning\n",
    "word embedding and data analytics methods and to extract knowledge from it. Prepare a report\n",
    "for this work and deposit it on moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this work you will use 20 Newsgroup dataset, but you a free to use any text data (UCI datasets\n",
    "repository, kaggle, data.gouv.fr, …) informing the Professor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work should contains at least the following 4 parts:\n",
    "1. Analysis of the text dataset\n",
    "2. Text processing and Transformation\n",
    "3. Apply di erent Neural Networks (NN) embedding techniques\n",
    "4. Clustering and/or classi cation on the embedded data\n",
    "5. Results analysis and visualisation\n",
    "6. Theoretical formalism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this work you will use 20 Newsgroup dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "Organization: The City University\n",
      "Lines: 14\n",
      "\n",
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "-- \n",
      "Michael Collier (Programmer)                 The Computer Unit,\n",
      "Email: M.P.Collier@uk.ac.city                The City University,\n",
      "Tel: 071 477-8000 x3769                      London,\n",
      "Fax: 071 477-8565                            EC1V 0HB.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyse the dataset : the context, size, difficulties, detect the objectives.\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "\n",
    "# Load the 20 newsgroups dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# the context of the dataset\n",
    "print(newsgroups_train.target_names)\n",
    "print(newsgroups_train.data[0])\n",
    "\n",
    "X_train = newsgroups_train.data\n",
    "Y_train = newsgroups_train.target\n",
    "\n",
    "X_test = newsgroups_test.data\n",
    "Y_test = newsgroups_test.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2257\n",
      "1502\n"
     ]
    }
   ],
   "source": [
    "# analyse the size of the dataset\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Text Processing and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Processing and Transformation\n",
    "# For this part, you should use scikit-learn and you can follow the tutorial:\n",
    "# https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#tutorial-setup\n",
    "\n",
    "# Assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices)\n",
    "\n",
    "# For each document #i, count the number of occurrences of each word w and store it in X[i, j] as the value of feature #j where j is the index of word w in the dictionary\n",
    "def build_X(data, dictionary):\n",
    "    X = np.zeros((len(data), len(dictionary)), dtype=np.int)\n",
    "    for i, doc in enumerate(data):\n",
    "        for word in doc.split():\n",
    "            X[i, dictionary[word]] += 1\n",
    "    return X\n",
    "\n",
    "\n",
    "def build_dictionary(data):\n",
    "    dictionary = {}\n",
    "    for doc in data:\n",
    "        for word in doc.split():\n",
    "            if word not in dictionary:\n",
    "                dictionary[word] = len(dictionary)\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "dictionary_train = build_dictionary(X_train)\n",
    "X_bow_train = build_X(X_train, dictionary_train)\n",
    "\n",
    "dictionary_test = build_dictionary(X_test)\n",
    "X_bow_test = build_X(X_test, dictionary_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(data, dictionary):\n",
    "#     vectorizer = CountVectorizer(vocabulary=dictionary)\n",
    "#     X = vectorizer.fit_transform(data)\n",
    "#     return X\n",
    "\n",
    "# X_cv_train = tokenize(X_train, dictionary_train)\n",
    "\n",
    "\n",
    "# print(X_cv_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Apply different embedding techniques based on Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 77)\t0.1501782300675281\n",
      "  (0, 76)\t0.1501782300675281\n",
      "  (0, 75)\t0.1501782300675281\n",
      "  (0, 74)\t0.103699862561056\n",
      "  (0, 73)\t0.12962948804367816\n",
      "  (0, 72)\t0.1501782300675281\n",
      "  (0, 71)\t0.1501782300675281\n",
      "  (0, 70)\t0.2851886015699785\n",
      "  (0, 69)\t0.11378139920465326\n",
      "  (0, 68)\t0.07044791025673265\n",
      "  (0, 67)\t0.1501782300675281\n",
      "  (0, 66)\t0.11378139920465326\n",
      "  (0, 65)\t0.12962948804367816\n",
      "  (0, 64)\t0.05877826376929799\n",
      "  (0, 63)\t0.1501782300675281\n",
      "  (0, 62)\t0.1501782300675281\n",
      "  (0, 61)\t0.080491850362952\n",
      "  (0, 60)\t0.035036250633131864\n",
      "  (0, 59)\t0.1501782300675281\n",
      "  (0, 58)\t0.09774341765798598\n",
      "  (0, 57)\t0.02293400626289806\n",
      "  (0, 56)\t0.06833288469445191\n",
      "  (0, 55)\t0.1330396798598353\n",
      "  (0, 54)\t0.08004648953595733\n",
      "  (0, 53)\t0.029526882905000593\n",
      "  :\t:\n",
      "  (4, 280)\t0.031709364894395534\n",
      "  (4, 278)\t0.023816220382475\n",
      "  (4, 273)\t0.04705982356268544\n",
      "  (4, 221)\t0.019082684263568732\n",
      "  (4, 219)\t0.0513555290675402\n",
      "  (4, 184)\t0.0281348399793534\n",
      "  (4, 182)\t0.01993829776038902\n",
      "  (4, 165)\t0.040513867783262845\n",
      "  (4, 151)\t0.05070099806068606\n",
      "  (4, 149)\t0.021054836950687533\n",
      "  (4, 132)\t0.047482076875433134\n",
      "  (4, 119)\t0.03244558492850022\n",
      "  (4, 111)\t0.03639881118195886\n",
      "  (4, 95)\t0.018215904489179598\n",
      "  (4, 53)\t0.023388805866655395\n",
      "  (4, 41)\t0.12701352003425828\n",
      "  (4, 39)\t0.06523825456438467\n",
      "  (4, 23)\t0.033675186165414255\n",
      "  (4, 22)\t0.048505605942652665\n",
      "  (4, 17)\t0.014875181122214124\n",
      "  (4, 14)\t0.02551169020197704\n",
      "  (4, 13)\t0.01568063409362937\n",
      "  (4, 7)\t0.11470047918738577\n",
      "  (4, 4)\t0.014816009052671312\n",
      "  (4, 0)\t0.014816009052671312\n"
     ]
    }
   ],
   "source": [
    "def tfidf(X):\n",
    "    transformer = TfidfTransformer()\n",
    "    X = transformer.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "X_tfidf_train = tfidf(X_bow_train)\n",
    "X_tfidf_test = tfidf(X_bow_test)\n",
    "\n",
    "print(X_tfidf_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ion\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.21886760e-01 -8.80167186e-02  5.61207712e-01 -3.76584500e-01\n",
      "  -5.05346417e-01 -8.75589192e-01  3.35634559e-01  9.01217103e-01\n",
      "  -3.27931195e-02  1.59283224e-02  1.15245506e-01 -1.64794326e+00\n",
      "  -2.54266858e-02  6.27756596e-01 -5.54876506e-01 -6.04082525e-01\n",
      "   5.49870133e-01 -7.30168283e-01 -5.06202042e-01 -1.54111886e+00\n",
      "   9.64176238e-01  1.64968562e+00  1.25206590e+00 -7.90552348e-02\n",
      "   3.56570929e-01 -2.71244720e-02 -4.82709289e-01 -2.53956109e-01\n",
      "  -1.19589233e+00 -5.02319276e-01  1.29548323e+00 -6.09193981e-01\n",
      "   1.70501685e+00 -1.06452310e+00 -2.71935642e-01  1.53095424e+00\n",
      "  -3.63216013e-01 -3.79867256e-01  1.09562874e+00 -1.18346441e+00\n",
      "  -3.10551357e-02 -4.05427396e-01 -7.71491408e-01  2.42090574e-03\n",
      "   8.88898894e-02  2.47664571e-01 -1.42922366e+00  8.83940339e-01\n",
      "  -1.36391833e-01  8.82265866e-01  4.79567312e-02 -2.19482020e-01\n",
      "  -5.07226467e-01 -1.92014024e-01  3.70096527e-02  1.45585850e-01\n",
      "   1.41330242e+00  2.11777538e-01 -7.45092332e-01  9.18270648e-02\n",
      "   1.15650699e-01  4.56272036e-01  8.32961779e-03 -3.09483297e-02\n",
      "  -5.46965972e-02  3.16372186e-01 -5.28616130e-01  2.57059425e-01\n",
      "  -1.00725734e+00  7.63952732e-01  5.26293933e-01 -2.05422997e-01\n",
      "   1.22020900e+00 -4.41721171e-01  1.27799106e+00 -9.53491747e-01\n",
      "   2.75265992e-01 -6.17212117e-01 -7.73554444e-01  9.24584508e-01\n",
      "  -7.32976258e-01 -2.32900735e-02 -5.79901874e-01  1.19345737e+00\n",
      "  -1.06042221e-01 -1.25406995e-01  4.33836818e-01  3.11382145e-01\n",
      "   1.11620986e+00  1.42027348e-01  1.09129071e+00  3.55849653e-01\n",
      "   5.72586298e-01 -4.34133738e-01  1.21113014e+00  1.02309573e+00\n",
      "  -6.64640427e-01 -2.40259305e-01 -2.68061906e-01 -3.85372937e-01]\n",
      " [-3.20092104e-02  2.82988787e-01  3.67835015e-01  6.64489746e-01\n",
      "  -4.15226817e-01 -1.00852549e+00  4.35627073e-01  1.31653643e+00\n",
      "  -6.00496054e-01 -9.11729336e-01 -4.34519887e-01 -1.93926322e+00\n",
      "   2.85899878e-01  4.23430115e-01  3.66050988e-01  7.34150857e-02\n",
      "   7.86633730e-01  1.97186977e-01 -3.93900394e-01 -1.45785320e+00\n",
      "   2.45441988e-01  3.43612313e-01  3.19421850e-02 -1.88928798e-01\n",
      "   5.88079929e-01  9.96982679e-02 -7.39342391e-01 -2.08016083e-01\n",
      "  -5.63553393e-01  1.49951428e-01  1.43636250e+00  5.81641018e-01\n",
      "   1.11104107e+00 -9.10148919e-01 -5.41465819e-01  6.19274318e-01\n",
      "   1.68915361e-01 -1.02480151e-01  3.20863962e-01 -6.96533263e-01\n",
      "   1.00358045e+00 -3.38652164e-01 -1.52556515e+00 -7.97856092e-01\n",
      "   4.50661391e-01 -2.87684798e-01 -1.09134400e+00  6.75851554e-02\n",
      "   1.35288879e-01  5.41148409e-02  6.52399838e-01 -2.69303590e-01\n",
      "  -5.59210777e-01 -6.87419772e-01 -1.82795674e-02  9.75957606e-03\n",
      "   8.12865973e-01 -5.05843163e-02  1.69561371e-01  1.44351101e+00\n",
      "  -1.43782487e-02 -4.90167886e-01 -4.43175733e-01  1.93472039e-02\n",
      "  -9.28165674e-01  7.30109870e-01 -8.77050087e-02 -1.48518011e-02\n",
      "  -9.84753847e-01  1.12539804e+00 -6.70126438e-01 -9.62075889e-01\n",
      "   1.35591769e+00 -4.63143051e-01  7.01388776e-01 -1.95142105e-01\n",
      "   7.59958208e-01 -8.78703415e-01 -1.01335955e+00  7.87675679e-01\n",
      "  -9.13697124e-01 -9.73387286e-02 -7.49864161e-01  1.41280007e+00\n",
      "  -6.34424806e-01 -1.15539022e-01  3.05580050e-01  3.15538108e-01\n",
      "   1.38067782e+00  5.82283556e-01  4.01383281e-01 -5.49130023e-01\n",
      "   8.76687169e-01 -7.94143230e-02  1.20819926e+00  9.98902321e-01\n",
      "   4.06464487e-01  1.93519831e-01  1.75976023e-01 -3.57400000e-01]\n",
      " [-5.34150004e-01  7.45588422e-01  4.54525530e-01  1.28792912e-01\n",
      "  -4.19024229e-01 -4.16180909e-01 -2.99234062e-01  1.38416469e+00\n",
      "  -7.38770008e-01 -7.81449616e-01 -5.70874214e-01 -1.54685938e+00\n",
      "   1.15675986e+00  6.73733056e-01  1.72724116e+00 -1.59071550e-01\n",
      "   3.43325675e-01 -3.27107340e-01 -4.09673631e-01 -1.57410300e+00\n",
      "   1.08659315e+00  1.34938943e+00  9.67157543e-01 -1.13436759e+00\n",
      "   5.78683138e-01 -1.95954919e-01 -2.92114198e-01 -2.98969269e-01\n",
      "  -6.59352958e-01 -7.81337798e-01  2.31972051e+00 -1.93133354e-01\n",
      "   8.53400826e-01 -1.72986135e-01 -4.89357024e-01  1.28662109e+00\n",
      "   1.64903164e-01 -2.57981807e-01  4.78192687e-01 -7.35223711e-01\n",
      "  -4.64087665e-01  2.40286067e-01 -9.31285560e-01 -7.16079772e-01\n",
      "   1.30569887e+00  4.87508506e-01 -1.30778301e+00  9.35124531e-02\n",
      "  -5.44560626e-02  5.92520118e-01  2.41951108e-01  2.53642350e-01\n",
      "  -1.62196481e+00 -1.67723909e-01  7.32292905e-02  9.71541882e-01\n",
      "   1.29145622e+00 -1.61691546e-01 -4.34211016e-01  8.73425364e-01\n",
      "  -9.25287046e-03  1.09348464e+00 -3.10436666e-01  7.71288812e-01\n",
      "  -8.80853713e-01  6.46898329e-01 -5.86649120e-01  3.31514001e-01\n",
      "  -1.11541069e+00  2.05202341e+00  2.04837158e-01 -3.06749582e-01\n",
      "   1.83137739e+00 -4.51453716e-01  1.55461133e-01 -1.69302538e-01\n",
      "   1.22512102e+00 -1.20240808e+00 -1.69030142e+00  9.18688059e-01\n",
      "  -5.60483932e-01 -8.85152459e-01 -2.16347551e+00  2.43307400e+00\n",
      "  -3.39523703e-01 -3.84582400e-01 -5.27588427e-01  2.22362682e-01\n",
      "   1.72867227e+00  4.94916260e-01 -3.76605839e-01 -5.55227578e-01\n",
      "   7.43261755e-01 -1.60844535e-01  1.68259060e+00  2.15743065e+00\n",
      "   6.60875365e-02 -1.85985580e-01  2.28712276e-01 -8.18901300e-01]\n",
      " [-4.65846062e-02  1.83052337e+00 -4.72396672e-01 -1.29520655e+00\n",
      "   1.66174099e-01 -9.59758937e-01  9.80994284e-01  1.56043911e+00\n",
      "  -6.09693110e-01 -1.36463165e+00  8.67420197e-01 -1.40654862e+00\n",
      "  -7.54279375e-01 -5.71548283e-01  1.25090873e+00 -9.09006417e-01\n",
      "   1.14789534e+00 -2.21772575e+00 -5.16136944e-01 -4.90970135e-01\n",
      "   1.16652012e+00  1.27174973e+00  2.37447786e+00  1.32132506e+00\n",
      "  -1.55816603e+00  3.13997149e-01 -4.09400314e-01 -1.60328686e-01\n",
      "   3.27090472e-01 -1.89997509e-01  5.98068476e-01 -7.52682567e-01\n",
      "   6.54280484e-01 -7.95739233e-01 -1.23505466e-01  1.59395123e+00\n",
      "   5.66994607e-01  3.09179783e-01 -9.44220960e-01  2.43247882e-01\n",
      "  -2.10629171e-03 -1.77590877e-01  3.70200545e-01  2.12304622e-01\n",
      "   7.07551599e-01 -6.42317533e-02  3.52031320e-01 -1.07120502e+00\n",
      "   1.43489406e-01  8.94411504e-01 -5.22075176e-01  2.03712340e-02\n",
      "  -9.38037097e-01 -4.52675670e-01  6.80304408e-01 -6.14200890e-01\n",
      "   6.81910753e-01 -1.00734169e-02 -4.67794806e-01 -4.85038459e-01\n",
      "  -5.82852997e-02  4.84421968e-01  8.79620314e-01  4.05345321e-01\n",
      "  -5.69607973e-01 -2.43015677e-01  3.46433409e-02  5.88147104e-01\n",
      "  -1.68835926e+00  1.08751930e-01  4.86947626e-01 -9.46696520e-01\n",
      "   1.19198835e+00  8.97705674e-01  4.43034738e-01 -1.81308791e-01\n",
      "  -1.78541631e-01  4.47418123e-01 -1.26805890e+00 -7.74895310e-01\n",
      "  -7.46524811e-01  3.62189204e-01 -6.58837795e-01  7.78803408e-01\n",
      "  -1.05614178e-01 -1.03414774e+00  9.63635862e-01  1.85645235e+00\n",
      "   4.15303856e-01 -4.46348608e-01 -4.52681571e-01 -2.53419697e-01\n",
      "   8.07398558e-01 -3.91981453e-01  1.11237407e+00  3.20239782e-01\n",
      "  -2.29492653e-02 -3.86232659e-02 -2.95672238e-01 -1.49940670e+00]\n",
      " [-1.34901392e+00  1.50367105e+00 -9.30743575e-01 -1.09465623e+00\n",
      "  -1.93486080e-01 -1.10069263e+00  5.19462764e-01  2.87181330e+00\n",
      "  -1.81837177e+00  1.43624306e-01 -7.45076388e-02 -8.14349890e-01\n",
      "  -9.09546793e-01  2.73069233e-01  5.95174097e-02 -1.32898307e+00\n",
      "   1.19332194e+00 -8.28857005e-01  2.47561699e-03 -3.39285469e+00\n",
      "   1.11935592e+00 -2.68389642e-01  2.47415066e+00  1.21897840e+00\n",
      "  -1.68153453e+00  1.16394937e+00 -1.08359909e+00  6.65992796e-01\n",
      "  -2.08681202e+00 -4.37365413e-01  2.48478198e+00  6.93316385e-02\n",
      "   1.88458931e+00 -2.48711276e+00  8.88975978e-01  7.26092160e-01\n",
      "  -5.76394022e-01 -2.57643797e-02 -1.23253584e+00 -8.67955148e-01\n",
      "   1.34018314e+00 -5.78774750e-01 -2.15571761e+00  1.10282779e+00\n",
      "   2.77289486e+00 -1.78175151e+00 -9.41727579e-01 -1.08880019e+00\n",
      "   5.99066556e-01 -3.37000906e-01 -4.01833892e-01  1.15696120e+00\n",
      "   1.24149585e+00 -1.22163880e+00  1.22316003e+00 -8.80193889e-01\n",
      "  -1.58772314e+00 -8.10411274e-01 -9.25112367e-01 -6.92841351e-01\n",
      "   1.10594642e+00 -1.77918494e+00  3.74473870e-01  9.36175108e-01\n",
      "  -1.40875745e+00  8.20618749e-01  1.16915452e+00  2.17330670e+00\n",
      "  -1.42155826e+00  3.40512586e+00 -5.95062435e-01  9.31770027e-01\n",
      "   1.82082212e+00  9.47268307e-01  1.89459479e+00  1.40780461e+00\n",
      "   6.24698520e-01  1.36522436e+00  1.30043089e-01 -1.80370700e+00\n",
      "  -2.71810365e+00 -7.38561749e-01 -8.49252582e-01  1.83558547e+00\n",
      "  -2.00089931e+00 -1.34369624e+00  1.97157669e+00  1.68575180e+00\n",
      "   3.97536218e-01 -4.67246860e-01  1.76884627e+00 -8.97253811e-01\n",
      "   6.79684520e-01 -8.63112569e-01  2.32321906e+00 -4.03063148e-01\n",
      "   5.09616494e-01  6.77865505e-01 -5.33346057e-01  5.19882850e-02]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "data = pd.DataFrame(X_train, columns=['Text'])\n",
    "# append X_test\n",
    "data = data.append(pd.DataFrame(X_test, columns=['Text']))\n",
    "\n",
    "def get_corpus(data):\n",
    "    corpus_text = 'n'.join(data[:1000]['Text'])\n",
    "    data = []\n",
    "    # iterate through each sentence in the file\n",
    "    for i in sent_tokenize(corpus_text):\n",
    "        temp = []\n",
    "        # tokenize the sentence into words\n",
    "        for j in word_tokenize(i):\n",
    "            temp.append(j.lower())\n",
    "        data.append(temp)\n",
    "    return data\n",
    "\n",
    "\n",
    "corpus = get_corpus(data)\n",
    "\n",
    "# Word2Vec\n",
    "model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# get the vector for each word in the vocabulary\n",
    "words_key_to_index = model.wv.key_to_index\n",
    "words_index_to_key = model.wv.index_to_key\n",
    "words_vectors = model.wv.vectors\n",
    "\n",
    "# split words_to_vectors into train and test\n",
    "X_w2v_train = words_vectors[:len(X_train)]\n",
    "X_w2v_test = words_vectors[len(X_train):]\n",
    "\n",
    "print(X_w2v_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02731436  0.03532854 -0.649094    0.017563    0.92363745 -0.20742893\n",
      "  -0.651582    1.2316182   0.6634129  -0.16187254  0.6303719  -0.784737\n",
      "  -0.38953945  1.4947654  -0.3791734   0.32408726  0.77620983 -0.6670806\n",
      "  -1.6654828  -1.3331561  -0.28174958 -0.24550638 -0.20765585 -1.4201847\n",
      "  -0.06489483 -0.47188443 -1.0115728  -1.5898122  -0.8926713   0.24811971\n",
      "  -0.7771993   0.7484617   0.6240752   0.9822568   1.0377414   1.188622\n",
      "   0.20870152 -0.49757436 -1.331943   -0.37159234  0.10683428  0.30106544\n",
      "  -0.69923186 -2.2436733  -0.29444557  0.22455022 -1.8583503  -0.470767\n",
      "  -0.46327186 -0.26368976  1.1957734   0.7683525  -0.43630293 -0.38898528\n",
      "   0.52679     0.39857554 -0.36295116 -1.0434092  -1.3003485  -0.02464187\n",
      "   1.0461676  -0.54038966  0.9614785   2.2792933  -0.35931805  0.45200264\n",
      "  -0.59067184 -0.35829046  0.5966126   1.4900286  -0.34155747  0.7667651\n",
      "   1.0837072  -0.39562172  1.7894056   0.34115708  0.42516664 -0.0836772\n",
      "   0.19626091  2.1668816   0.03700379 -0.8008802  -0.9224068   0.12881619\n",
      "  -0.20720567 -1.0346335   0.22506978  0.40205434 -1.1675119   0.5211276\n",
      "  -0.41916305  0.76718736  0.51165986  0.04905393 -0.21883148  0.69259894\n",
      "  -0.3855299   0.43453503 -0.48058224  1.1253182 ]\n",
      " [ 0.26488996 -0.12546995 -0.16788861  0.19333266  0.2846368   0.16947132\n",
      "  -0.60104847  1.3127248   0.6748746  -0.6758848   0.6696214  -0.34158322\n",
      "  -0.04746387  2.227613   -0.46771824  1.0383108   1.2430131   0.60618436\n",
      "  -0.789658   -1.6226449  -0.7431028  -0.4541652  -1.5401886  -1.2789145\n",
      "   0.5394311  -1.0299053  -0.71190536 -1.2334033  -0.7497963   0.64326876\n",
      "  -1.138788    0.7005016   1.5241395   0.9893168   1.0224292   1.0635035\n",
      "  -0.08234882 -0.43429804 -0.6370045  -1.0645142   0.13171418 -0.11844358\n",
      "  -0.9715175  -1.7648938   0.23407976  0.30144304 -1.3208246  -0.04748043\n",
      "   0.41449553 -0.56674004  1.1160204   0.7818137  -0.43060452 -1.120614\n",
      "   1.5072575  -0.0864105  -1.2428803  -1.5158341  -1.1617379  -0.08157074\n",
      "   0.49437708 -0.60949093  0.2107235   1.7976586  -0.66636777 -0.08979297\n",
      "  -0.7654648   0.07371356  0.915032    1.9321718  -0.1630884   0.9995908\n",
      "   1.9336638  -0.61300296  0.79044884 -0.22165585  0.9528131  -0.09123158\n",
      "   0.64128876  2.0380404  -0.45819873 -0.4995634  -1.7551323  -0.05788434\n",
      "  -0.36664817 -0.8599551   0.811662   -0.06347564 -0.35438374  0.6669333\n",
      "  -0.5992729   0.30272168  0.83991474  0.04427249 -0.53606546 -0.7134337\n",
      "  -0.11468021 -0.00965532 -0.06232605  1.7098026 ]\n",
      " [ 0.53015995 -0.9271702  -1.4231485  -0.07285105  2.0013444   0.40451247\n",
      "  -0.18156192 -0.70514256  0.17552392 -0.8531723   2.9245622  -0.77014494\n",
      "  -1.490035    2.12991     1.3643464   2.4415834   1.334829   -0.6279127\n",
      "  -0.33514166  0.14963408  0.40598705 -0.60581905 -1.1735547  -1.6947159\n",
      "   0.23005697 -2.2905886  -1.1703484  -3.212372   -1.1283046   0.41252348\n",
      "  -1.4065794   0.5466748   0.1961572   0.85449123  0.06325548  0.52430207\n",
      "   0.85971224 -0.47186455 -1.4270585  -0.7289959   1.5205799   1.7572416\n",
      "   0.23707958 -2.2107973  -0.15202074  1.8128308  -1.7632047  -0.8230287\n",
      "   0.83607304  0.84075826  0.8365698  -0.02677371  0.73786086 -0.9036729\n",
      "  -0.3216186  -0.30833277 -0.47403616 -0.1980275  -0.9703122   0.12561248\n",
      "   0.33961958  0.00735906  0.02542011  2.2545788  -0.61344254 -1.178549\n",
      "  -0.02575511 -1.1602777   0.6189191   0.8637919   0.24446242 -0.77316374\n",
      "   0.0162383   0.12400872  1.5570505  -0.8393346   0.5097617   1.2769206\n",
      "  -1.3300284   1.4612482   0.41617805  0.34520978 -2.3616567   0.28302047\n",
      "  -0.1089194  -0.48174152  0.1696044  -1.1631279   0.36842886 -0.09249485\n",
      "  -1.4692795  -0.22980894  0.07600796 -1.8947531  -1.9651213   0.22182499\n",
      "   1.1086656   0.03948087  0.48159817 -0.24678524]\n",
      " [-0.99816275  1.5489566  -2.3789284  -0.9956428   3.316299   -0.80351436\n",
      "  -1.2120785   0.26030943  1.0919151  -1.6132481   0.8144792  -0.48923942\n",
      "  -2.6454968  -0.41072604  1.6343968  -3.0007758   1.4157094  -3.7216284\n",
      "  -0.17065693  0.49897867  1.2699944   0.8866696   4.3088274  -0.1984704\n",
      "  -2.5023751   0.67177624 -3.975303    1.700773   -0.03520773  0.16077036\n",
      "  -1.0304925  -0.17553067  0.12887314 -0.48887736  3.0573232   1.7421632\n",
      "   2.2515934  -0.35256454 -3.2609549   1.7128006   0.29035172  0.54374504\n",
      "   2.5516222   0.8415277  -0.48962373 -0.81814635 -0.6981585  -1.5836897\n",
      "  -1.0788975   2.6525998   1.6970475   1.58128     0.72835034  0.7441922\n",
      "  -1.5350497  -0.3174678   1.6145688   1.5937104   2.9644387   0.32389417\n",
      "  -0.92176664  1.4461119   1.320704    2.4769125  -0.3159023   1.1001066\n",
      "   2.9033217   0.3322626  -0.7943574   1.0113484   0.19047242 -0.40987903\n",
      "  -0.72174484 -1.8439085   1.7991871  -0.17005563  1.0708486  -0.1281037\n",
      "  -1.8554417   0.54557776 -0.881781   -2.4545379   0.44741988  0.3627978\n",
      "   0.44389218 -0.38875753  1.2541487  -1.2809982  -1.8176237  -1.1808643\n",
      "   1.2749945  -0.48400593 -1.2416312   1.5505743  -0.6901263   2.292317\n",
      "  -1.4081378   0.85193247 -0.5776031  -2.447298  ]\n",
      " [-1.5623205   6.2614985  -3.8218935  -1.2890775  -1.7864783   2.031354\n",
      "  -3.029438    2.83776     4.1984673  -2.1310155  -1.1264162   3.0005748\n",
      "  -1.4705211   4.406824   -2.3213232  -6.543245    3.597703    1.9780157\n",
      "  -2.6050003  -5.156253   -3.248063    1.1423872  -1.9586358  -1.7882578\n",
      "  -3.6506011  -3.5432734  -2.725172    2.5004141   2.159914    2.3431516\n",
      "  -0.72865283  2.2696717   4.6941566  -1.6429567   5.3384604   2.7380614\n",
      "   0.76774156  2.1835523  -1.902756   -3.6416976   1.1383097  -3.9303524\n",
      "  -1.3335892  -3.403202   -0.02022605 -4.9252753   1.1157085   2.3054469\n",
      "   0.9608823  -1.7780794   2.6305711   2.284034   -1.265753   -4.4477863\n",
      "   1.6046418  -3.4414325  -3.4305673  -1.9235319  -0.34412497 -2.355908\n",
      "  -2.6431773  -0.84292674  1.4513085   2.7537231  -0.91912436  2.6934302\n",
      "   1.445278    1.7888982   1.444836    5.833046   -3.1312428   4.3200226\n",
      "   4.988876   -5.481575    3.786758    2.6699605   1.59606     0.17736773\n",
      "   3.0256705   3.7586122  -3.4027114  -2.8352168  -3.295859    1.7068807\n",
      "  -0.36662453 -2.5777156   6.919966    0.02000148 -2.3289118   0.09427373\n",
      "  -0.5741032   0.20412321  0.5020647   2.2296088   0.14935257  0.380494\n",
      "  -2.6520042  -0.10111715  0.9456358   3.5379763 ]]\n"
     ]
    }
   ],
   "source": [
    "# FastText\n",
    "from gensim.models import FastText\n",
    "\n",
    "\n",
    "model = FastText(corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "\n",
    "# get the vector for each word in the vocabulary\n",
    "words_key_to_index = model.wv.key_to_index\n",
    "words_index_to_key = model.wv.index_to_key\n",
    "words_vectors = model.wv.vectors\n",
    "\n",
    "# split words_to_vectors into train and test\n",
    "X_ft_train = words_vectors[:len(X_train)]\n",
    "X_ft_test = words_vectors[len(X_train):]\n",
    "\n",
    "print(X_ft_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.19194150e+00 -7.20928460e-02  1.37898371e-01 -7.25268364e-01\n",
      "  -1.04224706e+00 -4.24828410e-01  9.80469704e-01  1.66004837e+00\n",
      "   6.93412900e-01 -2.28604838e-01  1.53491354e+00 -1.31897795e+00\n",
      "  -4.73413199e-01  9.32623327e-01 -1.48994243e+00 -1.04747891e+00\n",
      "   8.04982603e-01 -5.29905975e-01 -1.80774406e-01 -3.22483420e-01\n",
      "   1.63759506e+00  1.40627408e+00  1.50395465e+00 -5.00040889e-01\n",
      "   3.96168739e-01 -2.32179329e-01  2.52919704e-01 -1.23326814e+00\n",
      "  -8.69860291e-01 -5.36870420e-01  2.50574499e-01 -4.36893046e-01\n",
      "   1.04510033e+00 -4.38539505e-01 -1.10824394e+00  1.00409007e+00\n",
      "   3.90145987e-01 -9.14870799e-01  1.37862968e+00 -1.62709486e+00\n",
      "  -9.08986628e-01  4.26964253e-01 -5.31982958e-01 -1.28990698e+00\n",
      "  -1.01923275e+00  1.19198096e+00 -7.74533212e-01  8.85437906e-01\n",
      "  -5.71339548e-01  1.61840749e+00 -9.20520797e-02 -2.94697762e-01\n",
      "   1.41870785e+00  1.22994065e+00 -6.24990523e-01 -9.57765996e-01\n",
      "   1.08501768e+00 -1.45115948e+00 -1.19974267e+00 -1.55025303e-01\n",
      "   1.77410424e-01  8.80907714e-01 -6.47903860e-01 -8.65464807e-01\n",
      "   1.45005476e+00  1.93259511e-02 -1.19826376e+00 -5.66271972e-03\n",
      "   8.13010558e-02 -3.44779670e-01  1.18960643e+00  2.22100437e-01\n",
      "   5.09020209e-01 -1.75384772e+00 -1.55494124e-01 -1.34924209e+00\n",
      "  -1.50104746e-01  2.26833582e-01 -1.00087333e+00  4.17277694e-01\n",
      "   1.02863240e+00 -5.47495902e-01  2.90667236e-01  8.60595167e-01\n",
      "  -1.08641557e-01  1.21631515e+00  6.35920227e-01 -5.63199401e-01\n",
      "  -4.81241286e-01  2.47278134e-03  1.45690143e+00  6.49721205e-01\n",
      "  -6.34861588e-02 -2.82123625e-01  1.21253216e+00  1.81367314e+00\n",
      "  -1.30502796e+00 -9.15711403e-01 -3.50163549e-01  5.89111149e-01]\n",
      " [-7.04781711e-01  1.34936556e-01  1.13224134e-01  9.85821009e-01\n",
      "  -6.26380265e-01 -1.64849305e+00  2.28423417e-01  2.37633801e+00\n",
      "  -2.28054911e-01 -8.33397806e-01  2.93455422e-01 -2.10533690e+00\n",
      "   2.13265076e-01  1.17143869e+00 -9.28469181e-01 -1.18324466e-01\n",
      "   1.50787723e+00 -4.30756420e-01 -8.11740160e-01  3.71145010e-01\n",
      "  -1.04116172e-01  2.58035600e-01  3.66071105e-01 -8.34347904e-01\n",
      "   1.01354945e+00 -3.72407436e-01 -3.34436297e-02 -2.70172477e-01\n",
      "  -5.84786773e-01  5.61056316e-01  1.58529902e+00  1.31526753e-01\n",
      "   1.98902577e-01 -4.53037620e-01 -1.98723292e+00  8.29118073e-01\n",
      "   6.26940072e-01 -5.50476611e-01  5.29908180e-01 -1.85910195e-01\n",
      "   1.02510393e-01  9.68421400e-02 -1.23422849e+00 -2.86148143e+00\n",
      "   4.82429981e-01  7.38055706e-01 -6.21923506e-01  2.93509632e-01\n",
      "  -6.55786812e-01  4.87665862e-01  5.28276861e-01 -1.10266042e+00\n",
      "  -1.78433552e-01  2.15463620e-02 -1.81510612e-01  6.18784353e-02\n",
      "   8.09836864e-01  1.62321068e-02  1.19417697e-01  7.09843934e-01\n",
      "  -6.13213360e-01 -8.96491349e-01 -8.43161523e-01 -5.83917610e-02\n",
      "  -2.16839656e-01 -7.17191920e-02 -3.46792728e-01 -1.68680573e+00\n",
      "  -1.04709700e-01 -1.35946333e-01  8.57993439e-02 -1.59262013e+00\n",
      "   9.47888076e-01  2.66693473e-01 -4.09920305e-01 -7.85331011e-01\n",
      "  -4.66156274e-01 -1.09435952e+00 -1.64818740e+00  1.79664624e+00\n",
      "  -6.05612516e-01  6.88667372e-02 -5.33497453e-01  1.52064514e+00\n",
      "  -2.81344503e-01  5.56788862e-01 -2.07714699e-02 -4.18428838e-01\n",
      "   1.03611636e+00  4.61637288e-01  1.54830322e-01  9.49961692e-02\n",
      "  -4.98502702e-03 -3.49506140e-01  1.65783703e+00  8.12243164e-01\n",
      "   4.63734806e-01 -9.14726630e-02  7.17474222e-02  3.56267869e-01]\n",
      " [-5.81445754e-01  1.17766678e+00 -4.98543978e-01 -1.17114842e+00\n",
      "  -7.71097600e-01 -6.10978842e-01 -1.25351942e+00  2.53642273e+00\n",
      "   6.36865258e-01 -9.71603572e-01  2.10078549e+00 -8.82321775e-01\n",
      "   1.06458890e+00 -9.54597369e-02  1.44426227e+00 -1.41948491e-01\n",
      "   1.30114818e+00  2.77781114e-02 -1.04521930e+00 -1.00661039e+00\n",
      "  -5.11649072e-01  3.28663349e-01  9.99048829e-01 -2.59608626e+00\n",
      "   1.22367764e+00 -1.13799679e+00  5.36501348e-01  3.62350732e-01\n",
      "  -2.46192425e-01 -1.35277951e+00  2.16437817e+00 -7.73554027e-01\n",
      "  -2.40701333e-01  7.97776699e-01 -2.08132553e+00  1.02390003e+00\n",
      "   3.37359011e-01 -8.41656327e-02  1.11112261e+00  2.09405363e-01\n",
      "  -1.11460948e+00  1.54615355e+00  6.43285334e-01 -2.19818687e+00\n",
      "   8.03546369e-01  1.02795875e+00 -2.18808442e-01  1.07298923e+00\n",
      "  -1.81087220e+00  9.11085248e-01  6.97893441e-01 -1.29934624e-01\n",
      "  -1.06214213e+00  3.45148206e-01 -1.29633760e+00  2.93133020e-01\n",
      "   1.39357936e+00  6.83657601e-02 -1.67381674e-01 -5.95571339e-01\n",
      "   7.62095228e-02  2.53603041e-01  4.55804199e-01  3.96296680e-01\n",
      "  -2.96073377e-01  1.22440720e+00 -3.32531691e-01 -1.09957159e+00\n",
      "  -1.05337489e+00  2.73498297e-01  1.14797831e+00  4.95074838e-02\n",
      "   2.44287992e+00  1.59865022e+00 -1.05328238e+00  9.40181464e-02\n",
      "   1.38026178e+00 -1.89834148e-01 -2.52188373e+00  1.50935650e+00\n",
      "   1.52007866e+00  2.88523167e-01 -2.11239672e+00  2.41721654e+00\n",
      "   1.02071166e+00 -2.51546472e-01 -1.24308753e+00 -1.38124633e+00\n",
      "   8.56865048e-01  1.26307809e+00 -1.18206358e+00  3.32589895e-01\n",
      "  -6.86840832e-01 -5.35075068e-01  2.48987913e+00  2.73727107e+00\n",
      "   2.41164237e-01 -2.63383687e-01  7.96462357e-01 -1.35168481e+00]\n",
      " [ 5.33058643e-01  1.77239680e+00 -5.70581496e-01 -1.18274331e+00\n",
      "   1.55817437e+00 -8.98252010e-01 -5.08252978e-02 -8.35177302e-02\n",
      "  -3.13992172e-01 -1.93018413e+00  1.07350552e+00 -1.86059320e+00\n",
      "  -1.07354534e+00 -1.14635384e+00  2.10256100e+00 -1.81937718e+00\n",
      "   3.13438058e-01 -2.65395451e+00 -9.67265785e-01  1.14825463e+00\n",
      "   1.49835467e-01  1.77529716e+00  2.21563268e+00  1.45453691e+00\n",
      "  -1.35330760e+00 -7.00175345e-01  2.76139557e-01 -3.90010864e-01\n",
      "   9.39470351e-01 -4.39784944e-01  2.84253955e-01 -8.21968853e-01\n",
      "   1.86973900e-01 -1.28976718e-01 -5.22679269e-01  1.58561611e+00\n",
      "   1.02877879e+00 -6.60741866e-01 -1.88714457e+00  1.00010550e+00\n",
      "  -5.09176515e-02 -8.32884163e-02  1.76708412e+00  1.07699722e-01\n",
      "   6.15822196e-01  1.50148857e+00  9.55249590e-04 -1.52848935e+00\n",
      "  -5.75665295e-01  9.67196584e-01  2.86545694e-01 -1.04221210e-01\n",
      "  -4.20715839e-01  4.81841713e-01  1.55984551e-01 -1.08182287e+00\n",
      "   7.12913692e-01 -6.26860440e-01 -3.48594016e-03 -1.82845876e-01\n",
      "   2.03013524e-01  1.10127032e+00  9.41330016e-01  7.71706760e-01\n",
      "   1.90927647e-02 -1.32044983e+00 -2.83530429e-02 -4.80279118e-01\n",
      "  -1.28296304e+00 -4.11580235e-01  1.91413438e+00 -6.83315516e-01\n",
      "   1.17182875e+00  1.98395801e+00 -9.57302928e-01 -5.89593053e-01\n",
      "  -1.14938498e+00  1.10225785e+00 -1.39320314e+00 -7.15085790e-02\n",
      "  -1.47817361e+00 -6.42384142e-02 -5.32822669e-01  5.91508806e-01\n",
      "   9.33964029e-02 -3.53410572e-01  4.08916205e-01  1.53787827e+00\n",
      "  -1.09178208e-01 -6.83803380e-01 -1.08199394e+00 -3.31318681e-03\n",
      "  -4.11892772e-01  5.01717210e-01  8.93821299e-01  4.71715540e-01\n",
      "  -2.02703190e+00 -1.55191019e-01 -3.83245945e-01 -1.47860181e+00]\n",
      " [-1.97818005e+00  1.99905503e+00 -1.59287310e+00 -1.45427489e+00\n",
      "   2.76467949e-01 -3.17955470e+00 -2.15201110e-01  2.23947763e+00\n",
      "  -8.13308299e-01  1.08169293e+00  6.87958837e-01 -7.83260822e-01\n",
      "  -1.09125888e+00 -6.38495564e-01 -7.58647382e-01 -9.78504539e-01\n",
      "   1.62350094e+00 -3.14387739e-01 -4.62187797e-01 -1.28981912e+00\n",
      "   5.41183770e-01  6.43817723e-01  2.19507217e+00  7.97463059e-01\n",
      "  -2.73273110e+00  1.62423193e+00 -1.43898976e+00  3.81951213e-01\n",
      "  -2.16993165e+00 -1.44900608e+00  2.62799668e+00 -6.20147765e-01\n",
      "   1.65629208e+00  3.96682695e-02  3.30219120e-02  1.81249642e+00\n",
      "  -1.60121703e+00  5.99989519e-02 -2.30051041e+00  1.19388175e+00\n",
      "   1.66170859e+00 -8.66317376e-02 -1.23530352e+00 -3.34157020e-01\n",
      "   2.36491728e+00 -1.38216937e+00 -1.65432942e+00 -2.01188612e+00\n",
      "  -2.78538406e-01 -1.89830196e+00  6.60280943e-01  5.93463600e-01\n",
      "   1.77327287e+00 -6.18810713e-01  1.30990994e+00  1.04993594e+00\n",
      "  -1.13992393e+00 -7.77306318e-01 -6.61016345e-01 -8.97071481e-01\n",
      "   1.24221885e+00 -1.45380056e+00  1.99467468e+00  1.39177251e+00\n",
      "   7.01260716e-02  3.78052056e-01  3.93637627e-01  6.16842568e-01\n",
      "  -8.31828177e-01  1.87347698e+00  1.22642112e+00  1.58647764e+00\n",
      "   1.83425176e+00  2.04892349e+00  2.01770043e+00  1.79717135e+00\n",
      "  -1.04850972e+00  2.46449399e+00 -2.24120334e-01 -4.41446722e-01\n",
      "  -2.13515377e+00  1.79068238e-01 -1.16223168e+00  2.15740991e+00\n",
      "  -3.10938692e+00 -5.35183966e-01  2.68102217e+00  1.20394409e+00\n",
      "   1.33517408e+00 -4.65059429e-01  1.14588428e+00 -2.26526156e-01\n",
      "  -2.54304260e-02 -1.58997440e+00  2.61411023e+00  9.91824418e-02\n",
      "   9.27609146e-01 -1.24748975e-01 -6.08419478e-01 -1.31522524e+00]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus)]\n",
    "model = Doc2Vec(documents, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "words_vectors = model.wv.vectors\n",
    "\n",
    "\n",
    "# split words_to_vectors into train and test\n",
    "X_d2v_train = words_vectors[:len(X_train)]\n",
    "X_d2v_test = words_vectors[len(X_train):]\n",
    "\n",
    "\n",
    "print(X_d2v_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101.   1.   1. ...   0.   0. 102.]\n",
      " [101.   1.   0. ...   0.   0. 102.]\n",
      " [101.   1.   0. ...   0.   0. 102.]\n",
      " [101.   1.   0. ...   0.   0. 102.]\n",
      " [101.   1.   0. ...   0.   0. 102.]]\n"
     ]
    }
   ],
   "source": [
    "# BERT model\n",
    "def build_bert_X(data):\n",
    "    from transformers import BertTokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    X = np.zeros((len(data), 768))\n",
    "    for i, doc in enumerate(data):\n",
    "        X[i] = tokenizer.encode(doc, add_special_tokens=True, max_length=768, pad_to_max_length=True)\n",
    "    return X\n",
    "\n",
    "X_bow_train_list = X_bow_train.tolist()\n",
    "X_bow_test_list = X_bow_test.tolist()\n",
    "\n",
    "X_bert_train = build_bert_X(X_bow_train_list)\n",
    "X_bert_test = build_bert_X(X_bow_train_list)\n",
    "\n",
    "print(X_bert_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Clustering and/or classification on the embedded data\n",
    "\n",
    "5. Results analysis and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real \t\t [2 2 2 0 3 0 1 3 2 2 1 3 2 3 1 0 1 3 0 0 2 3 1 0 2 1 1 3 2 0]\n",
      "Y_pred_w2v \t [2 2 0 0 2 0 3 2 2 0 1 1 3 3 2 3 1 2 0 2 0 3 0 2 0 2 1 0 2 2]\n",
      "Y_pred_ft \t [1 1 3 0 2 0 1 2 1 1 0 0 3 2 1 0 0 2 1 1 2 1 2 1 3 2 0 2 1 2]\n",
      "Y_pred_d2v \t [1 3 2 0 0 1 0 2 3 0 2 3 1 2 2 0 1 2 0 2 2 1 1 2 1 1 3 0 1 2]\n",
      "Y_pred_bert \t [1 1 3 3 3 3 3 2 2 2 3 1 0 0 1 1 2 0 3 0 3 0 3 1 1 1 3 3 2 2]\n",
      "w2v \t 0.278\n",
      "ft \t 0.24133333333333334\n",
      "d2v \t 0.2633333333333333\n",
      "bert \t 0.24333333333333335\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def knn(X_train, X_test, y_train, y_test):\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "\n",
    "# first 30 predictions\n",
    "\n",
    "print(\"real \\t\\t\", Y_test[:30])\n",
    "\n",
    "Y_pred_w2v = knn(X_w2v_train, X_w2v_test, Y_train, Y_test)\n",
    "print(\"Y_pred_w2v \\t\", Y_pred_w2v[0:30])\n",
    "\n",
    "Y_pred_ft = knn(X_ft_train, X_ft_test, Y_train, Y_test)\n",
    "print(\"Y_pred_ft \\t\", Y_pred_ft[0:30])\n",
    "\n",
    "Y_pred_d2v = knn(X_d2v_train, X_d2v_test, Y_train, Y_test)\n",
    "print(\"Y_pred_d2v \\t\", Y_pred_d2v[0:30])\n",
    "\n",
    "Y_pred_bert = knn(X_bert_train, X_bert_test, Y_train, Y_test)\n",
    "print(\"Y_pred_bert \\t\", Y_pred_bert[0:30])\n",
    "\n",
    "\n",
    "# calculate the goodness of fit\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def goodness_of_fit(y_test, y_pred):\n",
    "    return accuracy_score(y_test[0:1500], y_pred[0:1500])\n",
    "\n",
    "\n",
    "print(\"w2v \\t\", goodness_of_fit(Y_test, Y_pred_w2v))\n",
    "print(\"ft \\t\", goodness_of_fit(Y_test, Y_pred_ft))\n",
    "print(\"d2v \\t\", goodness_of_fit(Y_test, Y_pred_d2v))\n",
    "print(\"bert \\t\", goodness_of_fit(Y_test, Y_pred_bert))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analyzing the results given by different embedding techniques, you should be able to I observe that the best result is given by:\n",
    "\n",
    "    - Word2Vec\n",
    "\n",
    "The other three: FestText, Doc2Vec and Bert gave approximately the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Word2Vec Theoretical details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec -Skip-Gram\n",
    "\n",
    "One of the prominent techniques in representing the word as a vector is Word2Vec. It was created by a team of researchers at Google which was led by Tomas Mikolov. It is an unsupervised learning, neural network algorithms for obtaining vector representation of words. It is trained on a large corpus of text data (Wikipedia) which helps us recognize linear substructures of word vector space. (we will get to this once we get a hold on building the vector representations)\n",
    "\n",
    "Example:\n",
    "\n",
    "“The quick brown fox jumps over the lazy dog”\n",
    "\n",
    "Build a probability matrix of words w.r.t each word appearing in a fixed window size. Here, the window size implies the surroundings of a particular word. So, if the window size is 4, while constructing the probability matrix we consider four words on either side of a particular word.\n",
    "\n",
    "This variant of Word2Vec model is skip-gram model. This model helps us understand the contextual information of the whole sentence. In this model, we consider only one word at a time and build probability of all those words appearing in a fixed window size, around this word.\n",
    "\n",
    "Building a probability vector for the word ‘THE’ by calculating the probability of all other words appearing around it, for the example and then follow the same for the rest of the words. For this, it's need to be considered two things:\n",
    "\n",
    "1. The window size in consideration and\n",
    "\n",
    "2. The number of unique words appearing around the word “THE”.\n",
    "\n",
    "Here, for our convenience we consider the window size as 2. Now, the unique words appearing around the word “THE” in our small corpus are: quick, brown, jumps, over, lazy, dog. Hence, the probability of each word in the unique words appearing around “THE” becomes 1/6, whereas, the probability vector for the word “quick” is calculated by considering the unique words: the, brown and fox and hence becomes 1/3 for each of these words. This way the matrix is built for all other words in the corpus.\n",
    "\n",
    "![alt text for screen readers](m1.png \"Text to show on mouseover\")\n",
    "\n",
    "This probability vector for each word in the corpus becomes the target while training a neural network and the one hot embedding vector becomes the input layer in the model.\n",
    "\n",
    "![alt text for screen readers](m2.png \"Text to show on mouseover\")\n",
    "\n",
    "\n",
    "The weights of the hidden layer neurons after the model converges becomes our embedding vectors. Also, we can fix the length of the vector by deciding the number of neurons in the hidden layer while training.\n",
    "\n",
    "\n",
    "### Word2Vec -CBOW\n",
    "\n",
    "The other variant of Word2Vec model works on these same lines but with a slightly different approach. In this case, the input layer is not a one hot embedding vector of just one word but two-three words put together i.e. a sequence. Then the model is trained to predict the probability vector, same as in skip-gram model. Once the model converges, we get the weights of the hidden layer nuerons which are the required embeddings of a particular word. Since, we consider a sequence of inputs, it is called ‘continuous bag of words or CBOW’. This variant helps us predict the next word of the given input sequence with good accuracy.\n",
    "\n",
    "![alt text for screen readers](m3.png \"Text to show on mouseover\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ffc50327917d0330237aeed608577229a5182ed624d0b41d3fc311b6ae6efbd2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
